{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Detection with RESNET50 Transfer Learning\n",
    "\n",
    "This Jupyter Notebook demonstrates building a brain tumor detection model using the Resnet50 pre-trained convolutional neural network (CNN) for transfer learning. Transfer learning leverages the knowledge learned by Resnet50 on a massive image dataset (ImageNet) to accelerate your brain tumor detection model's training and potentially improve its performance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Import Necessary Libraries**\n",
    "2. **Load and Preprocess Brain Tumor Dataset**\n",
    "   - Download and prepare the brain tumor dataset\n",
    "   - Explore the dataset's content and structure (number of images, classes, etc.)\n",
    "   - Preprocess images: resizing, normalization, data augmentation (optional)\n",
    "3. **Prepare Dataloader**\n",
    "4. **Load the Resnet50 Model**\n",
    "   - Use the deep learning library FAST.AI to load the Resnet50 model pre-trained on ImageNet.\n",
    "   - Freeze the convolutional base layers of Resnet50 to retain their learned features.\n",
    "   - Add custom classification layers suitable for brain tumor detection.\n",
    "5. **Compile and Train the Model**\n",
    "  - Define loss function (e.g., binary cross-entropy for binary classification)\n",
    "   - Choose an optimizer (e.g., Adam)\n",
    "   - Specify metrics (e.g., accuracy)\n",
    "   - Train the model on the preprocessed brain tumor dataset\n",
    "   - Monitor training progress (loss, accuracy) using visualization tools\n",
    "   - Consider using techniques like early stopping or learning rate scheduling to optimize training\n",
    "6. **Evaluate the Model**\n",
    "   - Evaluate the model's performance on a separate test dataset that wasn't used for training.\n",
    "   - Calculate metrics like accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "7. **Visualize Results**\n",
    "   - Visualize intermediate activations using techniques like Grad-CAM to understand which image regions the model focuses on for classification.\n",
    "   - Generate predictions on new brain tumor images.\n",
    "\n",
    "**Additional Considerations:**\n",
    "\n",
    "- Experiment with hyperparameter tuning (e.g., learning rate, batch size) to improve model performance.\n",
    "- Consider using data augmentation techniques (e.g., random rotations, flips) to artificially increase the dataset size and improve model generalization.\n",
    "\n",
    "\n",
    "This project can contribute to the advancement of medical image analysis and potentially aid in early diagnosis of brain tumors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2QOkOxhI0ii"
   },
   "source": [
    "# 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade -q kaggle torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XjHo7EyAIvvD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import shutil\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "loXl1cz4JbW-"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Brain Tumor Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.upload()\n",
    "\n",
    "! mkdir ~/.kaggle\n",
    "\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d sartajbhuvaji/brain-tumor-classification-mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip /content/brain-tumor-classification-mri.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEpEspkEJnAt"
   },
   "source": [
    "# Create directoties for respective classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qE4BJL6bJkE0"
   },
   "outputs": [],
   "source": [
    "# Define source directories\n",
    "training_dir = '/content/Testing'\n",
    "testing_dir = '/content/Training'\n",
    "\n",
    "# Define target directories\n",
    "target_images_train_dir = 'Images/train'\n",
    "target_images_valid_dir = 'Images/valid'\n",
    "target_labels_train_dir = 'Labels/train'\n",
    "target_labels_valid_dir = 'Labels/valid'\n",
    "\n",
    "# Create target directories if they do not exist\n",
    "os.makedirs(target_images_train_dir, exist_ok=True)\n",
    "os.makedirs(target_images_valid_dir, exist_ok=True)\n",
    "os.makedirs(target_labels_train_dir, exist_ok=True)\n",
    "os.makedirs(target_labels_valid_dir, exist_ok=True)\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(src_dir, target_dir):\n",
    "    for tumor_type in os.listdir(src_dir):\n",
    "        tumor_src_dir = os.path.join(src_dir, tumor_type)\n",
    "        tumor_target_dir = os.path.join(target_dir, tumor_type)\n",
    "        os.makedirs(tumor_target_dir, exist_ok=True)\n",
    "\n",
    "        for filename in os.listdir(tumor_src_dir):\n",
    "            src_file = os.path.join(tumor_src_dir, filename)\n",
    "            target_file = os.path.join(tumor_target_dir, filename)\n",
    "            shutil.copyfile(src_file, target_file)\n",
    "\n",
    "# Copy training images\n",
    "copy_files(training_dir, target_images_train_dir)\n",
    "# Copy testing images as validation images\n",
    "copy_files(testing_dir, target_images_valid_dir)\n",
    "\n",
    "# Create empty label directories as YOLO requires label files for each image\n",
    "def create_empty_labels(image_dir, label_dir):\n",
    "    for tumor_type in os.listdir(image_dir):\n",
    "        tumor_image_dir = os.path.join(image_dir, tumor_type)\n",
    "        tumor_label_dir = os.path.join(label_dir, tumor_type)\n",
    "        os.makedirs(tumor_label_dir, exist_ok=True)\n",
    "\n",
    "        for filename in os.listdir(tumor_image_dir):\n",
    "            label_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "            label_file = os.path.join(tumor_label_dir, label_filename)\n",
    "            open(label_file, 'a').close()\n",
    "\n",
    "# Create empty labels for training and validation sets\n",
    "create_empty_labels(target_images_train_dir, target_labels_train_dir)\n",
    "create_empty_labels(target_images_valid_dir, target_labels_valid_dir)\n",
    "\n",
    "print(\"Dataset reorganization completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2DZ0QjIKqak"
   },
   "source": [
    "# Prepare Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the training and validation data\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "data_dir = '/content/Images'\n",
    "image_datasets = {x: datasets.ImageFolder(root=f'{data_dir}/{x}', transform=data_transforms[x]) for x in ['train', 'valid']}\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'valid']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open a file for writing in JSON format\n",
    "with open('class_names.json', 'w') as outfile:\n",
    "  # Dump the list of class names to the file\n",
    "  json.dump(class_names, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXjCr0d4Kysx"
   },
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "w0IWRVSaKzHg"
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained resnet model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "\n",
    "# Freeze all the layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Unfreeze the last two residual blocks (adjust as needed)\n",
    "for param in model.layer4[1].parameters():  # Module for block 2 after first conv\n",
    "    param.requires_grad = True\n",
    "for param in model.layer4[2:].parameters():  # Modules for block 3 onwards\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Modify the classifier to fit the number of classes\n",
    "num_classes = len(class_names)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "# Define the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Define the metrics\n",
    "accuracy = Accuracy(num_classes = num_classes, task='multiclass').to(device)\n",
    "precision = Precision(num_classes=num_classes, average='macro', task='multiclass').to(device)\n",
    "recall = Recall(num_classes=num_classes, average='macro', task='multiclass').to(device)\n",
    "f1_score = F1Score(num_classes=num_classes, average='macro', task='multiclass').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, save_filename='best_model.pth'):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    patience = 5  # patience for early stopping\n",
    "\n",
    "    history = {'train_loss': [], 'valid_loss': [], 'train_acc': [], 'valid_acc': [],\n",
    "               'train_precision': [], 'valid_precision': [], 'train_recall': [], 'valid_recall': [], 'train_f1': [], 'valid_f1': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_precision = 0.0\n",
    "            running_recall = 0.0\n",
    "            running_f1 = 0.0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_precision += precision(preds, labels)\n",
    "                running_recall += recall(preds, labels)\n",
    "                running_f1 += f1_score(preds, labels)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_precision = running_precision / len(dataloaders[phase])\n",
    "            epoch_recall = running_recall / len(dataloaders[phase])\n",
    "            epoch_f1 = running_f1 / len(dataloaders[phase])\n",
    "\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "            history[f'{phase}_precision'].append(epoch_precision.item())\n",
    "            history[f'{phase}_recall'].append(epoch_recall.item())\n",
    "            history[f'{phase}_f1'].append(epoch_f1.item())\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f} F1: {epoch_f1:.4f}')\n",
    "\n",
    "            if phase == 'valid':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    patience = 5  # Reset patience when new best accuracy is found\n",
    "                else:\n",
    "                    patience -= 1\n",
    "\n",
    "        print()\n",
    "\n",
    "        if patience == 0:\n",
    "            print(f'Early stopping triggered after epoch {epoch}.')\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Save the best model weights to a file\n",
    "    torch.save(best_model_wts, save_filename)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloaders):\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in dataloaders['valid']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    acc = running_corrects.double() / dataset_sizes['valid']\n",
    "    print(f'Validation Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the resnet model\n",
    "trained_model, training_history = train_model(model, criterion, optimizer, scheduler, num_epochs=25, save_filename = '_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(trained_model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning \n",
    "\n",
    "# Unfreeze all the layers\n",
    "for param in trained_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning\n",
    "fine_tuned_model, fine_tuning_history = train_model(trained_model, criterion, optimizer, scheduler, num_epochs=25, save_filename = 'fine_tuned_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(fine_tuned_model, dataloaders)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Brain_Tumor_ResNet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
